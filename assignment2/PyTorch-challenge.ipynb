{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. CIFAR-10 open-ended challenge\n",
    "\n",
    "\n",
    "### Things you might try:\n",
    "- **Filter size**: Above we used 5x5; would smaller filters be more efficient?\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use Dropout.\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
    "\n",
    "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from cs231n.train_utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F  # useful stateless functions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)\n",
    "\n",
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:cs231n.train_utils:optimize:\n",
      "DEBUG:cs231n.train_utils:coarse_step:\n",
      "DEBUG:cs231n.train_utils:train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "BEFORE\n",
      "fine = False\n",
      "sampled_all = False\n",
      "active = 0\n",
      "not active = False\n",
      "AFTER\n",
      "fine = False\n",
      "active = 0\n",
      "not active = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cs231n.train_utils:train: It 0, loss = 2.2912\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 11.28\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 10.90\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 100, loss = 2.0558\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 30.30\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 32.80\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 101, loss = 1.8820\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 32.64\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 34.00\n",
      "\n",
      "INFO:cs231n.train_utils:coarse_step:\tlr = 1.00E-03\n",
      "INFO:cs231n.train_utils:coarse_step:\t\tworking = True\n",
      "DEBUG:cs231n.train_utils:fine_step:\n",
      "DEBUG:cs231n.train_utils:train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE\n",
      "fine = False\n",
      "sampled_all = True\n",
      "active = 1\n",
      "not active = False\n",
      "AFTER\n",
      "fine = True\n",
      "active = 1\n",
      "not active = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cs231n.train_utils:train: It 0, loss = 2.0040\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 32.64\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 34.30\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 100, loss = 1.8750\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 37.85\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 40.60\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 200, loss = 1.7761\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 42.97\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 43.30\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 300, loss = 1.6279\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 47.48\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 46.00\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 400, loss = 1.3963\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 48.52\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 47.80\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 500, loss = 1.3130\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 52.60\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 50.40\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 600, loss = 1.5613\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 52.17\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 51.10\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 700, loss = 1.6302\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 55.90\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 53.00\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 0, loss = 1.2578\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 52.60\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 51.80\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 100, loss = 1.1119\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 56.51\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 52.10\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 200, loss = 1.3235\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 52.52\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 53.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 300, loss = 1.1035\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 56.08\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 51.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 400, loss = 1.3189\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 56.60\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 56.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 500, loss = 1.4177\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 55.82\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 54.00\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 600, loss = 1.2325\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 58.25\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 54.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 700, loss = 1.3764\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 60.07\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 55.90\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 0, loss = 1.2426\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 56.34\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 56.30\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 100, loss = 1.1296\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 56.25\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 53.30\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 200, loss = 1.1277\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 60.07\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 56.70\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 300, loss = 1.1453\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 61.63\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 57.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 400, loss = 0.8680\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 62.93\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 59.00\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 500, loss = 1.1689\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 61.63\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 58.60\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 600, loss = 1.2872\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 59.81\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 56.30\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 700, loss = 1.3848\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 59.55\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 57.20\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 0, loss = 0.8729\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 63.72\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 60.90\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 100, loss = 0.9288\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 63.80\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 61.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 200, loss = 1.0276\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 68.23\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 60.20\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 300, loss = 1.1441\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 66.75\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 61.20\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 400, loss = 1.0217\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 62.07\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 56.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 500, loss = 0.8887\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 66.67\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 61.80\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 600, loss = 1.1672\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 59.38\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 54.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 700, loss = 1.0096\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 63.63\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 61.10\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 0, loss = 1.0381\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 64.58\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 59.40\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 100, loss = 1.1162\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 65.45\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 62.00\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 200, loss = 1.0087\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 70.40\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 61.20\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 300, loss = 0.9125\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 66.75\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 61.50\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 400, loss = 0.9034\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 65.54\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 60.30\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 500, loss = 1.0316\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 63.37\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 58.20\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 600, loss = 0.9093\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 68.32\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 61.90\n",
      "\n",
      "INFO:cs231n.train_utils:train: It 700, loss = 1.0000\n",
      "INFO:cs231n.train_utils:train: Train acc \t= 61.28\n",
      "INFO:cs231n.train_utils:train: Val acc \t= 58.10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import os\n",
    "unicode = lambda x: x\n",
    "fs = unicode('%(asctime)s %(levelname)s:%(message)s',)\n",
    "ds = unicode('%b  %-d %H:%M:%S')\n",
    "logging.basicConfig(format=fs, datefmt=ds, level=logging.DEBUG)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(999)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #         \n",
    "# Experiment with any architectures, optimizers, and hyperparameters.          #\n",
    "# Achieve AT LEAST 70% accuracy on the *validation set* within 10 epochs.      #\n",
    "#                                                                              #\n",
    "# Note that you can use the check_accuracy function to evaluate on either      #\n",
    "# the test set or the validation set, by passing either loader_test or         #\n",
    "# loader_val as the second argument to check_accuracy. You should not touch    #\n",
    "# the test set until you have finished your architecture and  hyperparameter   #\n",
    "# tuning, and only run the test set once at the end to report a final value.   #\n",
    "################################################################################\n",
    "from cs231n.train_utils import *\n",
    "\n",
    "def plot(history):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.losses, 'o')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history.train_acc, '-o')\n",
    "    plt.plot(history.val_acc, '-o')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()\n",
    "\n",
    "im_shape = (3, 32, 32)\n",
    "num_classes = 10\n",
    "\n",
    "choices = {\n",
    "        \"Architecture\": [\"batchnorm-relu-conv\"],\n",
    "        \"FilterSize\": [3, 5],\n",
    "        #\"FilterCount\": [8, 32, 64],\n",
    "        \"FilterCount\": [8, 32],\n",
    "        #\"Stride\": [1, 2],\n",
    "        \"Stride\": [1, 2],\n",
    "        #\"N\": [3, 5, 10],\n",
    "        \"N\": [3],\n",
    "        #\"M\": [1, 2],\n",
    "        \"M\": [1],\n",
    "        \"HiddenSize\": [1000],\n",
    "        \"Dropout\": [0.25, 0.5, 0.95]\n",
    "}\n",
    "\n",
    "# This one should sample exactly the network from assignment.\n",
    "#\"\"\"\n",
    "choices = {\n",
    "        \"Architecture\": [\"batchnorm-relu-conv\"],\n",
    "        \"FilterSize\": [3],\n",
    "        \"FilterCount\": [32],\n",
    "        \"Stride\": [1],\n",
    "        \"N\": [2],\n",
    "        \"M\": [1],\n",
    "        \"HiddenSize\": [1000],\n",
    "        \"Dropout\": [0.]\n",
    "}\n",
    "#\"\"\"\n",
    "\n",
    "ho = HyperOpt(choices, construct_model, train, loader_train, loader_val,\n",
    "              max_active=3, coarse_its=100, fine_epochs=5, verbose=True,\n",
    "              visualize=True, port=6006, device=device)\n",
    "\n",
    "ho.optimize()\n",
    "\n",
    "#plot(history)\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe what you did"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set -- run this only once\n",
    "\n",
    "Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model). Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model\n",
    "check_accuracy_part34(loader_test, best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
